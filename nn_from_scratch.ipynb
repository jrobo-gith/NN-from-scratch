{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeec540546601f13",
   "metadata": {},
   "source": [
    "# Building a Neural Network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b813c674470212cc",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb1d8f023d755f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test = pd.read_csv('MNIST_CSV/mnist_test.csv')\n",
    "mnist_train = pd.read_csv('MNIST_CSV/mnist_train.csv')\n",
    "mnist_test = np.array(mnist_test)\n",
    "mnist_train = np.array(mnist_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288b0e0a3fe38859",
   "metadata": {},
   "source": [
    "## Test plot for a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e2bcc25711d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "randint = np.random.randint(0, mnist_train.shape[0])\n",
    "ax.imshow(np.reshape(mnist_train[randint, 1:], (28, 28)), cmap='Greys')\n",
    "ax.set_title(f\"{mnist_train[randint, 0]}\", fontsize=20)\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd05c83a3c6c3d90",
   "metadata": {},
   "source": [
    "### Change single label into array of 0s and 1s, (one hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67de3c595c323a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(label):\n",
    "    Y = np.zeros(10)\n",
    "    Y[label] = 1\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3bc6194cd57867",
   "metadata": {},
   "source": [
    "## Coding one Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7afc42485eb842",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, weight, biases, input):\n",
    "        self.weight = weight\n",
    "        self.biases = biases\n",
    "        self.input = input\n",
    "        self.output = 0 \n",
    "    \n",
    "    def forward(self) -> float:\n",
    "        self.output = np.dot(self.input, self.weight) + self.biases\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620dc44a57d922d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.uniform(0, 5, 5)\n",
    "b = np.random.uniform(0, 5, 5)\n",
    "X_data = np.random.uniform(0, 100, 5)\n",
    "\n",
    "neuron = Neuron(weight=w, biases=b, input=X_data)\n",
    "output = neuron.forward()\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6444d760ce98f802",
   "metadata": {},
   "source": [
    "## Whole Network of hidden layers and neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "424527fcdfd6539b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T15:23:11.763427Z",
     "start_time": "2025-04-22T15:23:11.738274Z"
    }
   },
   "outputs": [],
   "source": [
    "def Linear(x, weights, bias):\n",
    "    \"\"\"Linear hidden layer\"\"\"\n",
    "    return np.dot(x, weights) + bias\n",
    "\n",
    "def ReLU(inputs) -> np.ndarray:\n",
    "    \"\"\"Activation function\"\"\"\n",
    "    return np.maximum(0, inputs)\n",
    "\n",
    "def softMax(inputs: np.ndarray) -> np.ndarray:\n",
    "    exps = np.exp(inputs - inputs.max())\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def initialise_weights(num_HL, HL_structure, num_inputs, num_outputs) -> (np.ndarray, np.ndarray):\n",
    "    params = {\"W0\": np.random.normal(0, 2/num_inputs, size=(num_inputs, HL_structure[0]))}\n",
    "    for i in range(num_HL-1):\n",
    "        params[f\"W{i+1}\"] = np.random.normal(0, 2/HL_structure[i], size=(HL_structure[i], HL_structure[i+1]))\n",
    "    params[f\"W{num_HL}\"] = np.random.normal(0, 2/HL_structure[-1], size=(HL_structure[-1], num_outputs))\n",
    "    biases = np.zeros(num_HL+2)\n",
    "    return params, biases\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, num_inputs, hidden_layer_structure, num_outputs, lr=0.1):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden_layers = len(hidden_layer_structure)\n",
    "        self.hidden_layer_structure = hidden_layer_structure\n",
    "        self.num_outputs = num_outputs\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Randomly initialise weights and biases\n",
    "        self.weights, self.biases = initialise_weights(num_HL=self.num_hidden_layers,\n",
    "                                              HL_structure=self.hidden_layer_structure,\n",
    "                                              num_inputs=self.num_inputs,\n",
    "                                              num_outputs=self.num_outputs)\n",
    "        # Information I may want\n",
    "        self.num_nodes = self.num_hidden_layers + sum(hidden_layer_structure)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Zs, As = {}, {}\n",
    "        for i, weight_matrix in enumerate(self.weights.values()):\n",
    "            if i == 0:\n",
    "                Zs[f\"Z{i}\"] = Linear(x, weight_matrix, self.biases[i])\n",
    "                As[f\"A{i}\"] = ReLU(Zs[f\"Z{i}\"])\n",
    "            else:\n",
    "                Zs[f\"Z{i}\"] = Linear(As[f\"A{i-1}\"], weight_matrix, self.biases[i])\n",
    "                As[f\"A{i}\"] = ReLU(Zs[f\"Z{i}\"])\n",
    "    \n",
    "        output = softMax(Zs[f\"Z{i}\"])\n",
    "        return output, Zs, As\n",
    "    \n",
    "    def update_weights(self, grad_w, grad_b):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[f\"W{i}\"] -= self.lr * grad_w[f\"W{i}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "a726fd861417e98a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T15:23:12.167545Z",
     "start_time": "2025-04-22T15:23:12.165433Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy(outputs, targets):\n",
    "    \"\"\"Classification Loss Function\"\"\"\n",
    "    return - np.sum(softMax(targets) * np.log(outputs))\n",
    "\n",
    "def deriv_ReLU(x):\n",
    "    # ret = np.zeros_like(x)\n",
    "    # ret[x >= 0] = 1\n",
    "    # return ret\n",
    "    return x > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "10afaad7be9a906",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T15:23:12.466489Z",
     "start_time": "2025-04-22T15:23:12.458529Z"
    }
   },
   "outputs": [],
   "source": [
    "class Criterion:\n",
    "    def __init__(self, NN):\n",
    "        self.output = output\n",
    "        self.NN = NN\n",
    "        self.params = NN.weights\n",
    "        self.As = As\n",
    "        \n",
    "    def backward(self, output, Zs, As, one_hot_label):\n",
    "        one_hot_label = one_hot(one_hot_label)\n",
    "        m = len(one_hot_label)\n",
    "        \n",
    "        output_error = cross_entropy(output, one_hot_label)\n",
    "        \n",
    "        grad_w = {}\n",
    "        grad_b = {}\n",
    "        for i in range(1,len(As)):\n",
    "            # Hidden layers\n",
    "            layer = len(As) - i\n",
    "            if i == 1:\n",
    "                error = np.dot(self.params[f\"W{layer}\"], output_error * deriv_ReLU(Zs[f\"Z{layer}\"]))\n",
    "                dw = 1/m * np.dot(error[:, None], As[f\"A{layer}\"][:, None].T)\n",
    "                db = 1/m * np.sum(error[:, None], axis=1, keepdims=True)\n",
    "                \n",
    "                grad_w[f\"W{layer}\"] = dw\n",
    "                grad_b[f\"b{layer}\"] = db\n",
    "            else:\n",
    "                error = np.dot(self.params[f\"W{layer}\"], error.flatten() * deriv_ReLU(Zs[f\"Z{layer}\"]))\n",
    "                dw = 1/m * np.dot(error[:, None], As[f\"A{layer}\"][:, None].T)\n",
    "                db = 1/m * np.sum(error[:, None], axis=1, keepdims=True)\n",
    "                \n",
    "                grad_w[f\"W{layer}\"] = dw\n",
    "                grad_b[f\"b{layer}\"] = db\n",
    "        \n",
    "        error = np.dot(self.params[f\"W{layer-1}\"], error.flatten() * deriv_ReLU(Zs[f\"Z{layer-1}\"]))\n",
    "        dw = 1/m * np.dot(error[:, None], As[f\"A{layer-1}\"][:, None].T)\n",
    "        db = 1/m * np.sum(error[:, None], axis=1, keepdims=True)\n",
    "        grad_w[f\"W{layer-1}\"] = dw\n",
    "        grad_b[f\"b{layer-1}\"] = db\n",
    "        \n",
    "        return grad_w, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "c9f179d226cf3679",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T15:23:12.923706Z",
     "start_time": "2025-04-22T15:23:12.920381Z"
    }
   },
   "outputs": [],
   "source": [
    "rand_num = mnist_train[randint, 1:]\n",
    "true_label = one_hot(mnist_train[randint, 0])\n",
    "\n",
    "NN = NeuralNetwork(num_inputs=len(rand_num),\n",
    "                   hidden_layer_structure=[64],\n",
    "                   num_outputs=len(true_label))\n",
    "loss = Criterion(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "41fe2f04cbd9fcd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T15:23:32.923996Z",
     "start_time": "2025-04-22T15:23:13.293894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... | Error: 23.6 | Epoch 1/1 | Image 36000 / 59999\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[486], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m label \u001b[38;5;241m=\u001b[39m img[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m data \u001b[38;5;241m=\u001b[39m img[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m----> 7\u001b[0m output, Zs, As \u001b[38;5;241m=\u001b[39m NN\u001b[38;5;241m.\u001b[39mforward(data)\n\u001b[1;32m      8\u001b[0m grad_w, grad_b \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mbackward(output, Zs, As, label)\n\u001b[1;32m      9\u001b[0m NN\u001b[38;5;241m.\u001b[39mupdate_weights(grad_w\u001b[38;5;241m=\u001b[39mgrad_w, grad_b\u001b[38;5;241m=\u001b[39mgrad_b)\n",
      "Cell \u001b[0;32mIn[482], line 41\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, weight_matrix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 41\u001b[0m         Zs[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m Linear(x, weight_matrix, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[i])\n\u001b[1;32m     42\u001b[0m         As[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ReLU(Zs[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[482], line 3\u001b[0m, in \u001b[0;36mLinear\u001b[0;34m(x, weights, bias)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLinear\u001b[39m(x, weights, bias):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Linear hidden layer\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(x, weights) \u001b[38;5;241m+\u001b[39m bias\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, img in enumerate(mnist_train):\n",
    "        label = img[0]\n",
    "        data = img[1:]\n",
    "        output, Zs, As = NN.forward(data)\n",
    "        grad_w, grad_b = loss.backward(output, Zs, As, label)\n",
    "        NN.update_weights(grad_w=grad_w, grad_b=grad_b)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            error = np.round(cross_entropy(output, label), 2)\n",
    "            print(f\"Training... | Error: {error} | Epoch {epoch + 1}/{num_epochs} | Image {i} / {mnist_train.shape[0]}\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80152acf4e40a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
