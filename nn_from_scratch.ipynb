{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Building a Neural Network from scratch\n",
    "\n",
    "\n",
    "## Context\n",
    "A neural network (NN) is composed of layers. An $\\textit{input layer}$ where a single or vector of inputs can be fed into the NN, and an $\\textit{output layer}$, again the output could be singular (scalar) or contain a vector of outputs, and in the middle, $\\textit{hidden layers}$. These hidden layers are where the magic happens. Shallow NNs have one hidden layer with multiple $\\textit{nodes}$ whearas deep NNs contain multiple hidden layers.  \n",
    "\n",
    "## The magic\n",
    "As we said above, the hidden layers are filled with nodes. Each node has a $\\textit{weight}$ that comes with it which is updated with each pass back through the network (backpropagation). Each hidden layer also comes with a $\\textit{bias}$ that gets added to the weights at each layer. We train the NN by feeding through some training $x$ data, then use the NN, initialised with random weights, to predict an output $y$, whereupon a loss function will determine how \"correct\" the weights are. We then use the optimizer to perform backpropagation through the network to update the weights according to the loss function, whereupon we then feed through more $x$ data. We repeat the process until the loss function is below a certain tolerance.\n",
    "\n",
    "## Risks \n",
    "#### Overfitting\n",
    "Training the network by letting it go through the training data over and over again (epochs) is a good way to fit your training data well, but the network will likely not be $\\textit{generalisable}$ to your test data. This is why we have $\\textit{regularisation}$, where larger values for the loss function are punished to avoid the weights changing drastically. There are other ways to regularise NNs as well.\n",
    "\n",
    "#### Vanishing gradients\n",
    "This was an early problem in training deep neural networks, where each pass of backpropagation causes the gradients to become smaller and smaller, until the weights stop updating all together. To fix this, we introduce the $\\textit{ReLU}$ function, where the outputs of a layer are clamped to never go below 0, solving the vanishing gradients problem. \n",
    "\n",
    "<img src=\"figure/NN.png\" alt=\"NN\" width=\"500px\" style=\"display:flex;align-items:center;\">\n",
    "\n",
    "Figure 1: Typical NN with an input layer, three hidden layers, and an output layer.\n",
    "\n",
    "## This project\n",
    "In this project, we build a neural network from scratch, only using numpy to compute mathematical oporations such as ```np.dot(x,y)```. We will then train this neural network on the MNIST numbers dataset and evaluate its performance. If it does well, we can also evaluate it on the fashion MNIST dataset. "
   ],
   "id": "eeec540546601f13"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-20T09:44:56.906173Z",
     "start_time": "2025-04-20T09:44:56.304257Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 363
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Data",
   "id": "b813c674470212cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:55:31.115922Z",
     "start_time": "2025-04-20T09:55:28.436744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mnist_test = pd.read_csv('MNIST_CSV/mnist_test.csv')\n",
    "mnist_train = pd.read_csv('MNIST_CSV/mnist_train.csv')\n",
    "mnist_test = np.array(mnist_test)\n",
    "mnist_train = np.array(mnist_train)"
   ],
   "id": "87cb1d8f023d755f",
   "outputs": [],
   "execution_count": 388
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test plot for a number",
   "id": "288b0e0a3fe38859"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:55:51.010160Z",
     "start_time": "2025-04-20T09:55:50.961563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "randint = np.random.randint(0, mnist_train.shape[0])\n",
    "ax.imshow(np.reshape(mnist_train[randint, 1:], (28, 28)), cmap='Greys')\n",
    "ax.set_title(f\"{mnist_train[randint, 0]}\", fontsize=20)\n",
    "ax.axis('off')"
   ],
   "id": "12e2bcc25711d388",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 398
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Change single label into array of 0s and 1s, (one hot)",
   "id": "dd05c83a3c6c3d90"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:59:11.404567Z",
     "start_time": "2025-04-20T09:59:11.398161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def one_hot(label):\n",
    "    Y = np.zeros(10)\n",
    "    Y[label] = 1\n",
    "    return Y"
   ],
   "id": "67de3c595c323a1b",
   "outputs": [],
   "execution_count": 401
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Coding one Neuron",
   "id": "4e3bc6194cd57867"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T16:07:00.843323Z",
     "start_time": "2025-04-19T16:07:00.839853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Neuron:\n",
    "    def __init__(self, weight, biases, input):\n",
    "        self.weight = weight\n",
    "        self.biases = biases\n",
    "        self.input = input\n",
    "        self.output = 0 \n",
    "    \n",
    "    def forward(self) -> float:\n",
    "        self.output = np.dot(self.input, self.weight) + self.biases\n",
    "        return self.output"
   ],
   "id": "3e7afc42485eb842",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T16:07:30.399009Z",
     "start_time": "2025-04-19T16:07:30.388873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w = np.random.uniform(0, 5, 5)\n",
    "b = np.random.uniform(0, 5, 5)\n",
    "X_data = np.random.uniform(0, 100, 5)\n",
    "\n",
    "neuron = Neuron(weight=w, biases=b, input=X_data)\n",
    "output = neuron.forward()\n",
    "\n",
    "print(output)"
   ],
   "id": "620dc44a57d922d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[331.83507077 328.30188178 328.44918341 329.69167679 331.31690374]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Whole Network of hidden layers and neurons",
   "id": "6444d760ce98f802"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T16:07:39.801898Z",
     "start_time": "2025-04-19T16:07:39.798749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalise(weights: np.ndarray)-> np.ndarray:\n",
    "    w_max = np.max(weights)\n",
    "    w_min = np.min(weights)\n",
    "    return (weights - w_min) / (w_max - w_min)"
   ],
   "id": "343cebd54d8967d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T10:15:13.638552Z",
     "start_time": "2025-04-20T10:15:13.630587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def Linear(x, weights, bias):\n",
    "        \"\"\"Linear hidden layer\"\"\"\n",
    "        weights = normalise(weights)\n",
    "        return np.dot(x, weights) + bias\n",
    "\n",
    "def ReLU(inputs) -> np.ndarray:\n",
    "    \"\"\"Activation function\"\"\"\n",
    "    return np.maximum(0, inputs)\n",
    "\n",
    "def softMax(inputs: np.ndarray) -> np.ndarray:\n",
    "    exps = np.exp(inputs - inputs.max())\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def initialise_weights(num_HL, HL_structure, num_inputs, num_outputs) -> (np.ndarray, np.ndarray):\n",
    "    params = {\"W_inputs\": np.random.uniform(0, 1, size=(num_inputs, HL_structure[0]))}\n",
    "    for i in range(num_HL-1):\n",
    "        if i == num_HL-1:\n",
    "            params[f\"W{i+1}\"] = np.random.uniform(0, 1, size=(HL_structure[i], num_outputs))\n",
    "        else:\n",
    "            params[f\"W{i+1}\"] = np.random.uniform(0, 1, size=(HL_structure[i], HL_structure[i+1]))\n",
    "    params[\"W_outputs\"] = np.random.uniform(0, 1, size=(HL_structure[-1], num_outputs))\n",
    "    biases = np.zeros(num_HL+2) # +2 for input and output\n",
    "    biases[1:-1] = np.random.uniform(0, 1, num_HL) # Avoiding ends because inputs and outputs don't have biases\n",
    "    return params, biases\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, num_inputs, hidden_layer_structure, num_outputs):\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden_layers = len(hidden_layer_structure)\n",
    "        self.hidden_layer_structure = hidden_layer_structure\n",
    "        self.num_outputs = num_outputs\n",
    "        \n",
    "        # Randomly initialise weights and biases\n",
    "        self.weights, self.biases = initialise_weights(num_HL=self.num_hidden_layers,\n",
    "                                              HL_structure=self.hidden_layer_structure,\n",
    "                                              num_inputs=self.num_inputs,\n",
    "                                              num_outputs=self.num_outputs)\n",
    "        # Information I may want\n",
    "        self.num_nodes = self.num_hidden_layers + sum(hidden_layer_structure)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Zs, As = {}, {}\n",
    "        for i, weight_matrix in enumerate(self.weights.values()):\n",
    "            if i == 0:\n",
    "                Zs[f\"Z{i+1}\"] = Linear(x, weight_matrix, self.biases[i])\n",
    "                As[f\"A{i+1}\"] = ReLU(Zs[f\"Z{i+1}\"])\n",
    "            else:\n",
    "                Zs[f\"Z{i+1}\"] = Linear(Zs[f\"Z{i}\"], weight_matrix, self.biases[i])\n",
    "                As[f\"A{i+1}\"] = ReLU(Zs[f\"Z{i+1}\"])\n",
    "    \n",
    "        output = softMax(Zs[f\"Z{i+1}\"])\n",
    "        return output, Zs, As"
   ],
   "id": "424527fcdfd6539b",
   "outputs": [],
   "execution_count": 451
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T10:15:14.008706Z",
     "start_time": "2025-04-20T10:15:14.005041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cross_entropy(outputs, targets):\n",
    "    \"\"\"Classification Loss Function\"\"\"\n",
    "    return - np.sum(softMax(targets) * np.log(outputs))\n",
    "\n",
    "def deriv_ReLU(x):\n",
    "    return x > 0"
   ],
   "id": "a726fd861417e98a",
   "outputs": [],
   "execution_count": 452
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T10:15:14.288685Z",
     "start_time": "2025-04-20T10:15:14.284856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Loss:\n",
    "    def __init__(self, loss_function, params):\n",
    "        self.loss_function = loss_function\n",
    "        self.params = params\n",
    "    \n",
    "    def loss_func(self, outputs, targets):\n",
    "        return self.loss_function(outputs, targets)\n",
    "    \n",
    "    def backward(self):\n",
    "        for key, value in self.params.items():\n",
    "            print(key, value.shape)"
   ],
   "id": "10afaad7be9a906",
   "outputs": [],
   "execution_count": 453
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T10:15:59.600551Z",
     "start_time": "2025-04-20T10:15:59.571746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rand_num = mnist_train[randint, 1:]\n",
    "true_label = one_hot(mnist_train[randint, 0])\n",
    "\n",
    "NN = NeuralNetwork(num_inputs=len(rand_num),\n",
    "                   hidden_layer_structure=[128, 64, 32],\n",
    "                   num_outputs=len(true_label))\n",
    "\n",
    "loss = Loss(loss_function=cross_entropy, \n",
    "            params=NN.weights)\n",
    "\n",
    "outputs, Zs, As= NN.forward(X_data)\n",
    "print(outputs)\n",
    "print(true_label)\n",
    "# for key, value in Zs.items():\n",
    "#     print(key, value.shape)\n",
    "# loss.loss_func(outputs, true_label)"
   ],
   "id": "d885eb5146ce5612",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "execution_count": 457
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T10:06:32.236410Z",
     "start_time": "2025-04-20T10:06:32.235066Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2851f301589e247d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5bbddfd46b83561c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
