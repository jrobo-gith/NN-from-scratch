{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Building a Neural Network from scratch\n",
    "\n",
    "\n",
    "## Context\n",
    "A neural network (NN) is composed of layers. An $\\textit{input layer}$ where a single or vector of inputs can be fed into the NN, and an $\\textit{output layer}$, again the output could be singular (scalar) or contain a vector of outputs, and in the middle, $\\textit{hidden layers}$. These hidden layers are where the magic happens. Shallow NNs have one hidden layer with multiple $\\textit{nodes}$ whearas deep NNs contain multiple hidden layers.  \n",
    "\n",
    "## The magic\n",
    "As we said above, the hidden layers are filled with nodes. Each node has a $\\textit{weight}$ that comes with it which is updated with each pass back through the network (backpropagation). Each hidden layer also comes with a $\\textit{bias}$ that gets added to the weights at each layer. We train the NN by feeding through some training $x$ data, then use the NN, initialised with random weights, to predict an output $y$, whereupon a loss function will determine how \"correct\" the weights are. We then use the optimizer to perform backpropagation through the network to update the weights according to the loss function, whereupon we then feed through more $x$ data. We repeat the process until the loss function is below a certain tolerance.\n",
    "\n",
    "## Risks \n",
    "#### Overfitting\n",
    "Training the network by letting it go through the training data over and over again (epochs) is a good way to fit your training data well, but the network will likely not be $\\textit{generalisable}$ to your test data. This is why we have $\\textit{regularisation}$, where larger values for the loss function are punished to avoid the weights changing drastically. There are other ways to regularise NNs as well.\n",
    "\n",
    "#### Vanishing gradients\n",
    "This was an early problem in training deep neural networks, where each pass of backpropagation causes the gradients to become smaller and smaller, until the weights stop updating all together. To fix this, we introduce the $\\textit{ReLU}$ function, where the outputs of a layer are clamped to never go below 0, solving the vanishing gradients problem. \n",
    "\n",
    "<img src=\"figure/NN.png\" alt=\"NN\" width=\"500px\" style=\"display:flex;align-items:center;\">\n",
    "\n",
    "Figure 1: Typical NN with an input layer, three hidden layers, and an output layer.\n",
    "\n",
    "## This project\n",
    "In this project, we build a neural network from scratch, only using numpy to compute mathematical oporations such as ```np.dot(x,y)```. We will then train this neural network on the MNIST numbers dataset and evaluate its performance. If it does well, we can also evaluate it on the fashion MNIST dataset. "
   ],
   "id": "eeec540546601f13"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import OrderedDict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Data",
   "id": "b813c674470212cc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mnist_test = pd.read_csv('MNIST_CSV/mnist_test.csv')\n",
    "mnist_train = pd.read_csv('MNIST_CSV/mnist_train.csv')\n",
    "mnist_test = np.array(mnist_test)\n",
    "mnist_train = np.array(mnist_train)"
   ],
   "id": "87cb1d8f023d755f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test plot for a number",
   "id": "288b0e0a3fe38859"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "randint = np.random.randint(0, mnist_train.shape[0])\n",
    "ax.imshow(np.reshape(mnist_train[randint, 1:], (28, 28)), cmap='Greys')\n",
    "ax.set_title(f\"{mnist_train[randint, 0]}\", fontsize=20)\n",
    "ax.axis('off')"
   ],
   "id": "12e2bcc25711d388",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Change single label into array of 0s and 1s, (one hot)",
   "id": "dd05c83a3c6c3d90"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def one_hot(label):\n",
    "    Y = np.zeros(10)\n",
    "    Y[label] = 1\n",
    "    return Y"
   ],
   "id": "67de3c595c323a1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Coding one Neuron",
   "id": "4e3bc6194cd57867"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Neuron:\n",
    "    def __init__(self, weight, biases, input):\n",
    "        self.weight = weight\n",
    "        self.biases = biases\n",
    "        self.input = input\n",
    "        self.output = 0 \n",
    "    \n",
    "    def forward(self) -> float:\n",
    "        self.output = np.dot(self.input, self.weight) + self.biases\n",
    "        return self.output"
   ],
   "id": "3e7afc42485eb842",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "w = np.random.uniform(0, 5, 5)\n",
    "b = np.random.uniform(0, 5, 5)\n",
    "X_data = np.random.uniform(0, 100, 5)\n",
    "\n",
    "neuron = Neuron(weight=w, biases=b, input=X_data)\n",
    "output = neuron.forward()\n",
    "\n",
    "print(output)"
   ],
   "id": "620dc44a57d922d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Whole Network of hidden layers and neurons",
   "id": "6444d760ce98f802"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T15:23:11.763427Z",
     "start_time": "2025-04-22T15:23:11.738274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def Linear(x, weights, bias):\n",
    "    \"\"\"Linear hidden layer\"\"\"\n",
    "    return np.dot(x, weights) + bias\n",
    "\n",
    "def ReLU(inputs) -> np.ndarray:\n",
    "    \"\"\"Activation function\"\"\"\n",
    "    return np.maximum(0, inputs)\n",
    "\n",
    "def softMax(inputs: np.ndarray) -> np.ndarray:\n",
    "    exps = np.exp(inputs - inputs.max())\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def initialise_weights(num_HL, HL_structure, num_inputs, num_outputs) -> (np.ndarray, np.ndarray):\n",
    "    params = {\"W0\": np.random.normal(0, 2/num_inputs, size=(num_inputs, HL_structure[0]))}\n",
    "    for i in range(num_HL-1):\n",
    "        params[f\"W{i+1}\"] = np.random.normal(0, 2/HL_structure[i], size=(HL_structure[i], HL_structure[i+1]))\n",
    "    params[f\"W{num_HL}\"] = np.random.normal(0, 2/HL_structure[-1], size=(HL_structure[-1], num_outputs))\n",
    "    biases = np.zeros(num_HL+2)\n",
    "    return params, biases\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, num_inputs, hidden_layer_structure, num_outputs, lr=0.1):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden_layers = len(hidden_layer_structure)\n",
    "        self.hidden_layer_structure = hidden_layer_structure\n",
    "        self.num_outputs = num_outputs\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Randomly initialise weights and biases\n",
    "        self.weights, self.biases = initialise_weights(num_HL=self.num_hidden_layers,\n",
    "                                              HL_structure=self.hidden_layer_structure,\n",
    "                                              num_inputs=self.num_inputs,\n",
    "                                              num_outputs=self.num_outputs)\n",
    "        # Information I may want\n",
    "        self.num_nodes = self.num_hidden_layers + sum(hidden_layer_structure)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Zs, As = {}, {}\n",
    "        for i, weight_matrix in enumerate(self.weights.values()):\n",
    "            if i == 0:\n",
    "                Zs[f\"Z{i}\"] = Linear(x, weight_matrix, self.biases[i])\n",
    "                As[f\"A{i}\"] = ReLU(Zs[f\"Z{i}\"])\n",
    "            else:\n",
    "                Zs[f\"Z{i}\"] = Linear(As[f\"A{i-1}\"], weight_matrix, self.biases[i])\n",
    "                As[f\"A{i}\"] = ReLU(Zs[f\"Z{i}\"])\n",
    "    \n",
    "        output = softMax(Zs[f\"Z{i}\"])\n",
    "        return output, Zs, As\n",
    "    \n",
    "    def update_weights(self, grad_w, grad_b):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[f\"W{i}\"] -= self.lr * grad_w[f\"W{i}\"]"
   ],
   "id": "424527fcdfd6539b",
   "outputs": [],
   "execution_count": 482
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T15:23:12.167545Z",
     "start_time": "2025-04-22T15:23:12.165433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cross_entropy(outputs, targets):\n",
    "    \"\"\"Classification Loss Function\"\"\"\n",
    "    return - np.sum(softMax(targets) * np.log(outputs))\n",
    "\n",
    "def deriv_ReLU(x):\n",
    "    # ret = np.zeros_like(x)\n",
    "    # ret[x >= 0] = 1\n",
    "    # return ret\n",
    "    return x > 0"
   ],
   "id": "a726fd861417e98a",
   "outputs": [],
   "execution_count": 483
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T15:23:12.466489Z",
     "start_time": "2025-04-22T15:23:12.458529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Criterion:\n",
    "    def __init__(self, NN):\n",
    "        self.output = output\n",
    "        self.NN = NN\n",
    "        self.params = NN.weights\n",
    "        self.As = As\n",
    "        \n",
    "    def backward(self, output, Zs, As, one_hot_label):\n",
    "        one_hot_label = one_hot(one_hot_label)\n",
    "        m = len(one_hot_label)\n",
    "        \n",
    "        output_error = cross_entropy(output, one_hot_label)\n",
    "        \n",
    "        grad_w = {}\n",
    "        grad_b = {}\n",
    "        for i in range(1,len(As)):\n",
    "            # Hidden layers\n",
    "            layer = len(As) - i\n",
    "            if i == 1:\n",
    "                error = np.dot(self.params[f\"W{layer}\"], output_error * deriv_ReLU(Zs[f\"Z{layer}\"]))\n",
    "                dw = 1/m * np.dot(error[:, None], As[f\"A{layer}\"][:, None].T)\n",
    "                db = 1/m * np.sum(error[:, None], axis=1, keepdims=True)\n",
    "                \n",
    "                grad_w[f\"W{layer}\"] = dw\n",
    "                grad_b[f\"b{layer}\"] = db\n",
    "            else:\n",
    "                error = np.dot(self.params[f\"W{layer}\"], error.flatten() * deriv_ReLU(Zs[f\"Z{layer}\"]))\n",
    "                dw = 1/m * np.dot(error[:, None], As[f\"A{layer}\"][:, None].T)\n",
    "                db = 1/m * np.sum(error[:, None], axis=1, keepdims=True)\n",
    "                \n",
    "                grad_w[f\"W{layer}\"] = dw\n",
    "                grad_b[f\"b{layer}\"] = db\n",
    "        \n",
    "        error = np.dot(self.params[f\"W{layer-1}\"], error.flatten() * deriv_ReLU(Zs[f\"Z{layer-1}\"]))\n",
    "        dw = 1/m * np.dot(error[:, None], As[f\"A{layer-1}\"][:, None].T)\n",
    "        db = 1/m * np.sum(error[:, None], axis=1, keepdims=True)\n",
    "        grad_w[f\"W{layer-1}\"] = dw\n",
    "        grad_b[f\"b{layer-1}\"] = db\n",
    "        \n",
    "        return grad_w, grad_b"
   ],
   "id": "10afaad7be9a906",
   "outputs": [],
   "execution_count": 484
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T15:23:12.923706Z",
     "start_time": "2025-04-22T15:23:12.920381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rand_num = mnist_train[randint, 1:]\n",
    "true_label = one_hot(mnist_train[randint, 0])\n",
    "\n",
    "NN = NeuralNetwork(num_inputs=len(rand_num),\n",
    "                   hidden_layer_structure=[64],\n",
    "                   num_outputs=len(true_label))\n",
    "loss = Criterion(NN)"
   ],
   "id": "c9f179d226cf3679",
   "outputs": [],
   "execution_count": 485
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T15:23:32.923996Z",
     "start_time": "2025-04-22T15:23:13.293894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, img in enumerate(mnist_train):\n",
    "        label = img[0]\n",
    "        data = img[1:]\n",
    "        output, Zs, As = NN.forward(data)\n",
    "        grad_w, grad_b = loss.backward(output, Zs, As, label)\n",
    "        NN.update_weights(grad_w=grad_w, grad_b=grad_b)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            error = np.round(cross_entropy(output, label), 2)\n",
    "            print(f\"Training... | Error: {error} | Epoch {epoch + 1}/{num_epochs} | Image {i} / {mnist_train.shape[0]}\", end=\"\\r\")"
   ],
   "id": "41fe2f04cbd9fcd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... | Error: 23.6 | Epoch 1/1 | Image 36000 / 59999\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[486], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m label \u001B[38;5;241m=\u001B[39m img[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m      6\u001B[0m data \u001B[38;5;241m=\u001B[39m img[\u001B[38;5;241m1\u001B[39m:]\n\u001B[0;32m----> 7\u001B[0m output, Zs, As \u001B[38;5;241m=\u001B[39m NN\u001B[38;5;241m.\u001B[39mforward(data)\n\u001B[1;32m      8\u001B[0m grad_w, grad_b \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mbackward(output, Zs, As, label)\n\u001B[1;32m      9\u001B[0m NN\u001B[38;5;241m.\u001B[39mupdate_weights(grad_w\u001B[38;5;241m=\u001B[39mgrad_w, grad_b\u001B[38;5;241m=\u001B[39mgrad_b)\n",
      "Cell \u001B[0;32mIn[482], line 41\u001B[0m, in \u001B[0;36mNeuralNetwork.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, weight_matrix \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m---> 41\u001B[0m         Zs[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mZ\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m Linear(x, weight_matrix, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbiases[i])\n\u001B[1;32m     42\u001B[0m         As[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mA\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m ReLU(Zs[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mZ\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "Cell \u001B[0;32mIn[482], line 3\u001B[0m, in \u001B[0;36mLinear\u001B[0;34m(x, weights, bias)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mLinear\u001B[39m(x, weights, bias):\n\u001B[1;32m      2\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Linear hidden layer\"\"\"\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mdot(x, weights) \u001B[38;5;241m+\u001B[39m bias\n",
      "File \u001B[0;32m<__array_function__ internals>:200\u001B[0m, in \u001B[0;36mdot\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 486
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "80152acf4e40a7a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
