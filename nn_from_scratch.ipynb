{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Building a Neural Network from scratch\n",
    "\n",
    "\n",
    "## Context\n",
    "A neural network (NN) is composed of layers. An $\\textit{input layer}$ where a single or vector of inputs can be fed into the NN, and an $\\textit{output layer}$, again the output could be singular (scalar) or contain a vector of outputs, and in the middle, $\\textit{hidden layers}$. These hidden layers are where the magic happens. Shallow NNs have one hidden layer with multiple $\\textit{nodes}$ whearas deep NNs contain multiple hidden layers.  \n",
    "\n",
    "## The magic\n",
    "As we said above, the hidden layers are filled with nodes. Each node has a $\\textit{weight}$ that comes with it which is updated with each pass back through the network (backpropagation). Each hidden layer also comes with a $\\textit{bias}$ that gets added to the weights at each layer. We train the NN by feeding through some training $x$ data, then use the NN, initialised with random weights, to predict an output $y$, whereupon a loss function will determine how \"correct\" the weights are. We then use the optimizer to perform backpropagation through the network to update the weights according to the loss function, whereupon we then feed through more $x$ data. We repeat the process until the loss function is below a certain tolerance.\n",
    "\n",
    "## Risks \n",
    "#### Overfitting\n",
    "Training the network by letting it go through the training data over and over again (epochs) is a good way to fit your training data well, but the network will likely not be $\\textit{generalisable}$ to your test data. This is why we have $\\textit{regularisation}$, where larger values for the loss function are punished to avoid the weights changing drastically. There are other ways to regularise NNs as well.\n",
    "\n",
    "#### Vanishing gradients\n",
    "This was an early problem in training deep neural networks, where each pass of backpropagation causes the gradients to become smaller and smaller, until the weights stop updating all together. To fix this, we introduce the $\\textit{ReLU}$ function, where the outputs of a layer are clamped to never go below 0, solving the vanishing gradients problem. \n",
    "\n",
    "<img src=\"figure/NN.png\" alt=\"NN\" width=\"500px\" style=\"display:flex;align-items:center;\">\n",
    "\n",
    "Figure 1: Typical NN with an input layer, three hidden layers, and an output layer.\n",
    "\n",
    "## This project\n",
    "In this project, we build a neural network from scratch, only using numpy to compute mathematical oporations such as ```np.dot(x,y)```. We will then train this neural network on the MNIST numbers dataset and evaluate its performance. If it does well, we can also evaluate it on the fashion MNIST dataset. "
   ],
   "id": "eeec540546601f13"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-20T09:44:56.906173Z",
     "start_time": "2025-04-20T09:44:56.304257Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 363
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Data",
   "id": "b813c674470212cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:55:31.115922Z",
     "start_time": "2025-04-20T09:55:28.436744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mnist_test = pd.read_csv('MNIST_CSV/mnist_test.csv')\n",
    "mnist_train = pd.read_csv('MNIST_CSV/mnist_train.csv')\n",
    "mnist_test = np.array(mnist_test)\n",
    "mnist_train = np.array(mnist_train)"
   ],
   "id": "87cb1d8f023d755f",
   "outputs": [],
   "execution_count": 388
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test plot for a number",
   "id": "288b0e0a3fe38859"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:55:51.010160Z",
     "start_time": "2025-04-20T09:55:50.961563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "randint = np.random.randint(0, mnist_train.shape[0])\n",
    "ax.imshow(np.reshape(mnist_train[randint, 1:], (28, 28)), cmap='Greys')\n",
    "ax.set_title(f\"{mnist_train[randint, 0]}\", fontsize=20)\n",
    "ax.axis('off')"
   ],
   "id": "12e2bcc25711d388",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 398
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Change single label into array of 0s and 1s, (one hot)",
   "id": "dd05c83a3c6c3d90"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T09:59:11.404567Z",
     "start_time": "2025-04-20T09:59:11.398161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def one_hot(label):\n",
    "    Y = np.zeros(10)\n",
    "    Y[label] = 1\n",
    "    return Y"
   ],
   "id": "67de3c595c323a1b",
   "outputs": [],
   "execution_count": 401
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Coding one Neuron",
   "id": "4e3bc6194cd57867"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T16:07:00.843323Z",
     "start_time": "2025-04-19T16:07:00.839853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Neuron:\n",
    "    def __init__(self, weight, biases, input):\n",
    "        self.weight = weight\n",
    "        self.biases = biases\n",
    "        self.input = input\n",
    "        self.output = 0 \n",
    "    \n",
    "    def forward(self) -> float:\n",
    "        self.output = np.dot(self.input, self.weight) + self.biases\n",
    "        return self.output"
   ],
   "id": "3e7afc42485eb842",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T16:07:30.399009Z",
     "start_time": "2025-04-19T16:07:30.388873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w = np.random.uniform(0, 5, 5)\n",
    "b = np.random.uniform(0, 5, 5)\n",
    "X_data = np.random.uniform(0, 100, 5)\n",
    "\n",
    "neuron = Neuron(weight=w, biases=b, input=X_data)\n",
    "output = neuron.forward()\n",
    "\n",
    "print(output)"
   ],
   "id": "620dc44a57d922d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[331.83507077 328.30188178 328.44918341 329.69167679 331.31690374]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Whole Network of hidden layers and neurons",
   "id": "6444d760ce98f802"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T16:07:39.801898Z",
     "start_time": "2025-04-19T16:07:39.798749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def normalise(weights: np.ndarray)-> np.ndarray:\n",
    "#     w_max = np.max(weights)\n",
    "#     w_min = np.min(weights)\n",
    "#     return (weights - w_min) / (w_max - w_min)"
   ],
   "id": "343cebd54d8967d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T11:50:52.227941Z",
     "start_time": "2025-04-20T11:50:52.215104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def Linear(x, weights, bias):\n",
    "    \"\"\"Linear hidden layer\"\"\"\n",
    "    return np.dot(x, weights) + bias\n",
    "\n",
    "def ReLU(inputs) -> np.ndarray:\n",
    "    \"\"\"Activation function\"\"\"\n",
    "    return np.maximum(0, inputs)\n",
    "\n",
    "def softMax(inputs: np.ndarray) -> np.ndarray:\n",
    "    exps = np.exp(inputs - inputs.max())\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def initialise_weights(num_HL, HL_structure, num_inputs, num_outputs) -> (np.ndarray, np.ndarray):\n",
    "    params = {\"W_inputs\": np.random.normal(0, np.sqrt(2/num_inputs), size=(num_inputs, HL_structure[0]))}\n",
    "    for i in range(num_HL-1):\n",
    "        if i == num_HL-1:\n",
    "            params[f\"W{i+1}\"] = np.random.normal(0, np.sqrt(2/HL_structure[i]), size=(HL_structure[i], num_outputs))\n",
    "        else:\n",
    "            params[f\"W{i+1}\"] = np.random.normal(0, np.sqrt(2/HL_structure[i]), size=(HL_structure[i], HL_structure[i+1]))\n",
    "    params[\"W_outputs\"] = np.random.normal(0, np.sqrt(2/HL_structure[-1]), size=(HL_structure[-1], num_outputs))\n",
    "    biases = np.zeros(num_HL+2)\n",
    "    return params, biases\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, num_inputs, hidden_layer_structure, num_outputs):\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_hidden_layers = len(hidden_layer_structure)\n",
    "        self.hidden_layer_structure = hidden_layer_structure\n",
    "        self.num_outputs = num_outputs\n",
    "        \n",
    "        # Randomly initialise weights and biases\n",
    "        self.weights, self.biases = initialise_weights(num_HL=self.num_hidden_layers,\n",
    "                                              HL_structure=self.hidden_layer_structure,\n",
    "                                              num_inputs=self.num_inputs,\n",
    "                                              num_outputs=self.num_outputs)\n",
    "        # Information I may want\n",
    "        self.num_nodes = self.num_hidden_layers + sum(hidden_layer_structure)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Zs, As = {}, {}\n",
    "        for i, weight_matrix in enumerate(self.weights.values()):\n",
    "            if i == 0:\n",
    "                Zs[f\"Z{i}\"] = Linear(x, weight_matrix, self.biases[i])\n",
    "                As[f\"A{i}\"] = ReLU(Zs[f\"Z{i}\"])\n",
    "            else:\n",
    "                Zs[f\"Z{i}\"] = Linear(As[f\"A{i-1}\"], weight_matrix, self.biases[i])\n",
    "                As[f\"A{i}\"] = ReLU(Zs[f\"Z{i}\"])\n",
    "    \n",
    "        output = softMax(As[f\"A{i}\"])\n",
    "        return output, Zs, As"
   ],
   "id": "424527fcdfd6539b",
   "outputs": [],
   "execution_count": 519
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T11:50:52.730353Z",
     "start_time": "2025-04-20T11:50:52.728370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cross_entropy(outputs, targets):\n",
    "    \"\"\"Classification Loss Function\"\"\"\n",
    "    return - np.sum(softMax(targets) * np.log(outputs))\n",
    "\n",
    "def deriv_ReLU(x):\n",
    "    return x > 0"
   ],
   "id": "a726fd861417e98a",
   "outputs": [],
   "execution_count": 520
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T11:50:53.097302Z",
     "start_time": "2025-04-20T11:50:53.094656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Loss:\n",
    "    def __init__(self, loss_function, params):\n",
    "        self.loss_function = loss_function\n",
    "        self.params = params\n",
    "    \n",
    "    def loss_func(self, outputs, targets):\n",
    "        return self.loss_function(outputs, targets)\n",
    "    \n",
    "    def backward(self):\n",
    "        for key, value in self.params.items():\n",
    "            print(key, value.shape)"
   ],
   "id": "10afaad7be9a906",
   "outputs": [],
   "execution_count": 521
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T11:50:53.651801Z",
     "start_time": "2025-04-20T11:50:53.627578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rand_num = mnist_train[randint, 1:]\n",
    "true_label = one_hot(mnist_train[randint, 0])\n",
    "\n",
    "NN = NeuralNetwork(num_inputs=len(rand_num),\n",
    "                   hidden_layer_structure=[128, 64, 32],\n",
    "                   num_outputs=len(true_label))\n",
    "\n",
    "loss = Loss(loss_function=cross_entropy, \n",
    "            params=NN.weights)\n",
    "outputs, Zs, As= NN.forward(X_data)\n",
    "print(outputs)\n",
    "for key, value in Zs.items():\n",
    "    print(key, value)\n",
    "# loss.loss_func(outputs, true_label)"
   ],
   "id": "d885eb5146ce5612",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.21664830e-016 4.66921519e-122 4.66921519e-122 4.66921519e-122\n",
      " 9.99530975e-001 9.65697258e-021 4.66921519e-122 9.14220528e-100\n",
      " 4.69025165e-004 4.66921519e-122]\n",
      "Z0 [ -11.35425514  165.67503783   88.67836334  343.93584736  -96.74052741\n",
      "  -19.72234126  196.76977     384.95645055  331.80560938  -30.42158817\n",
      "  216.60352699  335.55540941  301.60805501 -143.61612489   56.55748531\n",
      "  247.30707829   72.54768433  -74.57861423  235.41727791  152.44417933\n",
      " -162.07946751 -133.29004518  202.34318439  -35.45506419 -148.67265236\n",
      "  306.58308005   88.37181219  186.96845437   38.95464591  151.43947697\n",
      "  176.99526243 -205.14001566  -39.51082679  -19.92015281 -246.53296129\n",
      " -309.10277018  306.06426949  108.09931318 -405.48730966  -58.58748332\n",
      " -561.49726975  313.02395458  486.31206472  271.03269334  115.30427555\n",
      " -268.15964942  -28.80468504  275.88040722  161.83592453 -296.06832017\n",
      " -153.42167275  302.14257391 -252.46826944  128.34274307  385.66977367\n",
      " -199.08175505  295.60436434  136.63821149  -40.62457639  183.27472759\n",
      "  185.41963387  214.42743157 -186.94039633   55.8378828     9.36654388\n",
      "   30.23297342  -46.43943717 -187.9272101  -455.76157818 -171.70513887\n",
      "  222.14164356  -46.90067778   25.05581167  102.82393747  145.60485676\n",
      "  -91.25713871 -357.08549534  117.30022396    3.12741368  397.28144252\n",
      "  184.68760475   56.37412998  -83.71617555  -64.68159636 -172.35724541\n",
      "  -46.11225321  145.59302452  293.10491895  208.69724358  118.72684048\n",
      "  -40.22353211  105.70186852  -53.60408737 -104.01425325 -206.51802822\n",
      "  -72.57147375 -104.80315713  145.86852941 -248.68667413 -163.75690193\n",
      "  -90.14306614   22.62465787 -308.7591934  -172.86044262  -67.01538156\n",
      "  206.61542436   63.96689475  246.69431163 -201.54757178  151.38766327\n",
      "  445.45124328   80.42387328   -8.76974254 -162.54411996   21.26797766\n",
      "  -50.61889869 -188.99752519 -168.99462356 -279.57958378  110.95642054\n",
      " -115.09022038   47.82086791 -206.00841225   -9.82732777  250.24893008\n",
      " -137.4033171   -71.55537075  -60.04792528]\n",
      "Z1 [-1.04186114e+02  5.00140960e+02  6.50871260e+01  2.84559894e+02\n",
      " -1.70529644e+02 -4.29158822e+02  2.55816126e+02  3.66045651e+01\n",
      " -3.33594272e+02  6.49031030e+01  1.17469415e+02 -9.05047929e+01\n",
      "  1.14409864e+02  2.37145056e+02 -2.57679492e+02  3.04102770e+02\n",
      " -8.04423208e+01  2.71647393e+01  3.94553265e+02  7.83332688e+01\n",
      "  7.75314978e+01  6.08935121e+01  1.30398274e+02 -1.46900903e+02\n",
      " -3.80488962e+01 -4.99532716e+01  4.63165096e+02  1.21415127e+02\n",
      " -3.38171817e+02  3.53619047e+02  3.82230170e+02  2.27358163e+02\n",
      "  1.02583041e+02  1.76889379e+02 -1.37181011e+02  1.79856761e+01\n",
      "  2.11855873e+01  4.88319642e+01  2.64293630e+01 -2.64586895e+01\n",
      " -2.84131895e+01 -7.19935143e+01 -5.22762051e+01  2.01308516e+02\n",
      "  3.93388067e+02  3.95960295e+02  8.35926594e+01  8.32523097e+00\n",
      "  4.04366627e+01  2.50220625e+01  1.43913436e+02 -3.39327619e+02\n",
      "  1.77058071e+02 -1.74288655e+02  3.08235605e+02 -4.07841333e+02\n",
      "  2.22916576e+02 -3.46264401e+01  1.50247865e+02 -5.48636317e+02\n",
      " -2.65027207e-01  5.49946464e+01 -1.18065007e+02 -1.10312578e+02]\n",
      "Z2 [ 223.8599317  -249.68721946   71.40203008  284.2125652   209.76850355\n",
      " -124.22224639   16.20542565 -205.50407744 -123.73219033  184.59139417\n",
      "  552.3307109   -94.8654689    66.43002959   10.27980641   92.64821969\n",
      "   35.93255354 -745.29581492  251.61928735 -542.89978928 -185.48265026\n",
      "  132.25386105   72.32153742 -396.42266858  -16.6482144    75.52784844\n",
      "  327.8951674   581.66832376 -116.56735751  -52.67745705  -88.26458225\n",
      "   23.60667295 -502.66340925]\n",
      "Z3 [ 244.36025976 -169.56536882 -428.13793175 -402.63497787  279.37392121\n",
      "  233.28778359 -284.95879853   51.32878268  271.70953621 -321.55547679]\n"
     ]
    }
   ],
   "execution_count": 522
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T10:06:32.236410Z",
     "start_time": "2025-04-20T10:06:32.235066Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2851f301589e247d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5bbddfd46b83561c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
